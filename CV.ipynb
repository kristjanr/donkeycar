{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for a in ['', '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python37.zip',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/lib-dynload',\n",
    "          '/Users/kristjan.roosild/.local/lib/python3.7/site-packages',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages',\n",
    "          '/Users/kristjan.roosild/projects/donkeycar',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/locket-0.2.1-py3.7.egg']:\n",
    "    sys.path.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________             ______                   _________              \n",
      "___  __ \\_______________  /___________  __    __  ____/_____ ________\n",
      "__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n",
      "_  /_/ // /_/ /  / / /  ,<  /  __/  /_/ /     / /___  / /_/ /_  /    \n",
      "/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n",
      "                                 /____/                              \n",
      "\n",
      "using donkey v4.3.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import donkeycar as dk\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "from donkeycar.config import Config\n",
    "from donkeycar.parts.keras import KerasPilot\n",
    "from donkeycar.pipeline.database import PilotDatabase\n",
    "from donkeycar.pipeline.sequence import TubRecord, TubSequence, TfmIterator\n",
    "from donkeycar.pipeline.types import TubDataset\n",
    "from donkeycar.pipeline.augmentations import ImageAugmentation\n",
    "from donkeycar.utils import normalize_image, train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchSequence(object):\n",
    "    \"\"\"\n",
    "    The idea is to have a shallow sequence with types that can hydrate\n",
    "    themselves to np.ndarray initially and later into the types required by\n",
    "    tf.data (i.e. dictionaries or np.ndarrays).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: KerasPilot,\n",
    "                 config: Config,\n",
    "                 records: List[TubRecord],\n",
    "                 is_train: bool) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.sequence = TubSequence(records)\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = ImageAugmentation(config, 'AUGMENTATIONS')\n",
    "        self.transformation = ImageAugmentation(config, 'TRANSFORMATIONS')\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(len(self.pipeline) / self.batch_size)\n",
    "\n",
    "    def image_processor(self, img_arr):\n",
    "        \"\"\" Transformes the images and augments if in training. Then\n",
    "            normalizes it. \"\"\"\n",
    "        img_arr = self.transformation.run(img_arr)\n",
    "        if self.is_train:\n",
    "            img_arr = self.augmentation.run(img_arr)\n",
    "        norm_img = normalize_image(img_arr)\n",
    "        return norm_img\n",
    "\n",
    "    def _create_pipeline(self) -> TfmIterator:\n",
    "        \"\"\" This can be overridden if more complicated pipelines are\n",
    "            required \"\"\"\n",
    "\n",
    "        # 1. Initialise TubRecord -> x, y transformations\n",
    "        def get_x(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting x from record for training\"\"\"\n",
    "            out_tuple = self.model.x_transform_and_process(\n",
    "                record, self.image_processor)\n",
    "            # convert tuple to dictionary which is understood by tf.data\n",
    "            out_dict = self.model.x_translate(out_tuple)\n",
    "            return out_dict\n",
    "\n",
    "        def get_y(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting y from record for training \"\"\"\n",
    "            y0 = self.model.y_transform(record)\n",
    "            y1 = self.model.y_translate(y0)\n",
    "            return y1\n",
    "\n",
    "        # 2. Build pipeline using the transformations\n",
    "        pipeline = self.sequence.build_pipeline(x_transform=get_x,\n",
    "                                                y_transform=get_y)\n",
    "        return pipeline\n",
    "\n",
    "    def create_tf_data(self) -> tf.data.Dataset:\n",
    "        \"\"\" Assembles the tf data pipeline \"\"\"\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator=lambda: self.pipeline,\n",
    "            output_types=self.model.output_types(),\n",
    "            output_shapes=self.model.output_shapes())\n",
    "        return dataset.repeat().batch(self.batch_size)\n",
    "\n",
    "\n",
    "def get_model_train_details(database: PilotDatabase, model: str = None) -> Tuple[str, int]:\n",
    "    if not model:\n",
    "        model_name, model_num = database.generate_model_name()\n",
    "    else:\n",
    "        model_name, model_num = os.path.abspath(model), 0\n",
    "    return model_name, model_num\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config file: /Users/kristjan.roosild/mycar/config.py\n",
      "loading personal config over-rides from myconfig.py\n"
     ]
    }
   ],
   "source": [
    "MYCAR_PATH = '/Users/kristjan.roosild/mycar/'\n",
    "DATA_PATH = '/Users/kristjan.roosild/OneDrive/kool/maka/data/'\n",
    "MODELS_LOCATION = '/Users/kristjan.roosild/OneDrive/kool/maka/models/'\n",
    "\n",
    "cfg = dk.load_config(config_path=MYCAR_PATH + 'config.py')\n",
    "\n",
    "cfg.SEQUENCE_LENGTH = SEQUENCE_LENGTH\n",
    "cfg.WANDB_ENABLED = False\n",
    "\n",
    "\n",
    "def load_records(tub_name, model, cfg, size=None):\n",
    "    return TubDataset(\n",
    "        config=cfg,\n",
    "        tub_paths=[os.path.expanduser(DATA_PATH + tub_name)],\n",
    "        seq_size=model.seq_size()).get_records()[:size]\n",
    "\n",
    "def load_data(cfg, model, size=None):\n",
    "    tub_names_80_speed = [\n",
    "        '1-1-CC-80',\n",
    "        '2-1-CW-80',\n",
    "        '4-1-CC-80',\n",
    "    ]\n",
    "    tub_names_85_speed = [\n",
    "        '1-3-CC-85',\n",
    "        '2-3-CW-85',\n",
    "        '3-3-CW-85',\n",
    "        '4-3-CC-85'\n",
    "    ]\n",
    "    tub_names_90_speed = [\n",
    "        '1-2-CC-90',\n",
    "        '2-2-CW-90',\n",
    "        '3-2-CW-90',\n",
    "        '4-2-CC-90',\n",
    "    ]\n",
    "\n",
    "\n",
    "    tub_records_80_speed = {tn: load_records(tn, model, cfg, size) for tn in tub_names_80_speed}\n",
    "    tub_records_85_speed = {tn: load_records(tn, model, cfg, size) for tn in tub_names_85_speed}\n",
    "    tub_records_90_speed = {tn: load_records(tn, model, cfg, size) for tn in tub_names_90_speed}\n",
    "\n",
    "    return tub_records_80_speed, tub_records_85_speed, tub_records_90_speed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "\n",
    "def prep_fold_data(kl, cfg, data):\n",
    "    training_records, validation_records = train_test_split(data, shuffle=True,\n",
    "                                                            test_size=(1. - cfg.TRAIN_TEST_SPLIT))\n",
    "    print(f'Records # Training {len(training_records)}')\n",
    "    print(f'Records # Validation {len(validation_records)}')\n",
    "    # We need augmentation in validation when using crop / trapeze\n",
    "    training_pipe = BatchSequence(kl, cfg, training_records, is_train=True)\n",
    "    validation_pipe = BatchSequence(kl, cfg, validation_records, is_train=False)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset_train = training_pipe.create_tf_data().prefetch(tune)\n",
    "    dataset_validate = validation_pipe.create_tf_data().prefetch(tune)\n",
    "    train_size = len(training_pipe)\n",
    "    val_size = len(validation_pipe)\n",
    "    assert val_size > 0, \"Not enough validation data, decrease the batch size or add more data.\"\n",
    "    return dataset_train, dataset_validate, train_size, val_size\n",
    "\n",
    "\n",
    "def init_wandb(tub, kl):\n",
    "    import wandb\n",
    "    config = {\n",
    "        \"tub\": tub,\n",
    "        \"model\": str(kl)\n",
    "    }\n",
    "    wandb.init(project=\"master-thesis\", entity=\"kristjan\", config=config)\n",
    "\n",
    "\n",
    "def train(kl, model_path, cfg, data):\n",
    "    dataset_train, dataset_validate, train_size, val_size = prep_fold_data(kl, cfg, data)\n",
    "    history = kl.train(model_path=model_path,\n",
    "                       train_data=dataset_train,\n",
    "                       train_steps=train_size,\n",
    "                       batch_size=cfg.BATCH_SIZE,\n",
    "                       validation_data=dataset_validate,\n",
    "                       validation_steps=val_size,\n",
    "                       epochs=cfg.MAX_EPOCHS,\n",
    "                       verbose=cfg.VERBOSE_TRAIN,\n",
    "                       min_delta=cfg.MIN_DELTA,\n",
    "                       patience=cfg.EARLY_STOP_PATIENCE,\n",
    "                       show_plot=cfg.SHOW_PLOT,\n",
    "                       add_wandb_callback=cfg.WANDB_ENABLED)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return np.mean((np.array(v1) - np.array(v2)) ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def get_loss(kl, test_records):\n",
    "\n",
    "    if model.seq_size() > 0:\n",
    "        ground_truth = [r[model.seq_size()-1].underlying['user/angle'] for r in test_records]\n",
    "    else:\n",
    "        ground_truth = [r.underlying['user/angle'] for r in test_records]\n",
    "\n",
    "    pipe = BatchSequence(kl, cfg, test_records, is_train=False)\n",
    "    steps = len(pipe)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset = pipe.create_tf_data().prefetch(tune)\n",
    "    test_preds = kl.interpreter.model.predict(\n",
    "        dataset,\n",
    "        workers=2,\n",
    "        use_multiprocessing=True,\n",
    "        steps=steps,\n",
    "        verbose=1)[0]\n",
    "    return mse(ground_truth, test_preds)\n",
    "\n",
    "def get_test_metrics(model, fold, tub_records_80_speed, tub_records_85_speed, records_90_speed):\n",
    "    test_losses_for_80_speed = {}\n",
    "    test_losses_for_85_speed = {}\n",
    "    test_losses_for_90_speed = {}\n",
    "\n",
    "    print(f'Getting 80-speed mse-s for fold {fold}')\n",
    "    for test_tub_name, test_records in tub_records_80_speed.items():\n",
    "        mse_80_speed = get_loss(model, test_records)\n",
    "        test_losses_for_80_speed[test_tub_name] = mse_80_speed\n",
    "        print(f'80 speed mse for test tub {test_tub_name} is {mse_80_speed}')\n",
    "\n",
    "    print(f'Getting 85-speed mse-s for fold {fold}')\n",
    "    for test_tub_name, test_records in tub_records_85_speed.items():\n",
    "        mse_85_speed = get_loss(model, test_records)\n",
    "        test_losses_for_85_speed[test_tub_name] = mse_85_speed\n",
    "        print(f'85 speed mse for test tub {test_tub_name} is {mse_85_speed}')\n",
    "\n",
    "    print(f'Getting 90-speed mse-s for fold {fold}')\n",
    "\n",
    "    mse_90_speed = get_loss(model, records_90_speed)\n",
    "    test_losses_for_90_speed[f'90_speed_{fold}'] = mse_90_speed\n",
    "    print(f'90 speed mse for fold {fold} is {mse_90_speed}')\n",
    "\n",
    "    return test_losses_for_80_speed, test_losses_for_85_speed, test_losses_for_90_speed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, fold: str, train_records: list, cfg: Config):\n",
    "    model_name = f'{str(model)}-fold-{fold}'\n",
    "    model_path = f'{MODELS_LOCATION}{model_name}.h5'\n",
    "    train(model, model_path, cfg, train_records)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.parts.keras:input_shape (120, 160, 3) num_outputs 1\n",
      "INFO:donkeycar.parts.keras:Created Keras3D_CNNOnlySteering with interpreter: KerasInterpreter\n",
      "INFO:donkeycar.parts.keras:Num outputs 1\n",
      "INFO:donkeycar.parts.keras:Sequence length 3\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-1-CC-80']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-1-CW-80']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-1-CC-80/catalog_4.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-1-CW-80/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-1-CC-80']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-3-CC-85']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-1-CC-80/catalog_5.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-3-CC-85/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-3-CW-85']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-3-CW-85/catalog_6.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/3-3-CW-85']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/3-3-CW-85/catalog_4.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-3-CC-85/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-3-CC-85']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-2-CC-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-2-CC-90/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-2-CW-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-2-CW-90/catalog_2.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/3-2-CW-90/catalog_6.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/3-2-CW-90']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-2-CC-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-2-CC-90/catalog_7.catalog\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 3\n",
    "model = dk.parts.keras.Keras3D_CNNOnlySteering(seq_length=SEQUENCE_LENGTH)\n",
    "tub_records_80_speed, tub_records_85_speed, tub_records_90_speed = load_data(cfg, model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "17223"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "all_90_speed_data = list(itertools.chain(*list(tub_records_90_speed.values())))\n",
    "len(all_90_speed_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5,  shuffle=True, random_state=42)\n",
    "losses = defaultdict(dict)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(all_90_speed_data)):\n",
    "    train_records = [all_90_speed_data[i] for i in train_index]\n",
    "    test_records = [all_90_speed_data[i] for i in test_index]\n",
    "\n",
    "    trained_model = train_model(model, fold, train_records, cfg)\n",
    "\n",
    "    test_losses_for_80_speed, test_losses_for_85_speed, test_losses_for_90_speed = get_test_metrics(\n",
    "        trained_model, fold, tub_records_80_speed, tub_records_85_speed, test_records\n",
    "    )\n",
    "\n",
    "    losses.update(test_losses_for_80_speed)\n",
    "    losses.update(test_losses_for_85_speed)\n",
    "    losses.update(test_losses_for_90_speed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(dict,\n            {'1-1-CC-80': 0.0023009018791333435,\n             '2-1-CW-80': 0.001708727721087877,\n             '4-1-CC-80': 0.0020456999340228263,\n             '1-3-CC-85': 0.002089439282745076,\n             '2-3-CW-85': 0.001844253616628544,\n             '3-3-CW-85': 0.0019366432261592181,\n             '4-3-CC-85': 0.002072817969503371,\n             '2-2-CW-90': 0.001847796032144411,\n             '3-2-CW-90': 0.0019082155468252981,\n             '4-2-CC-90': 0.00041997193532866683,\n             '1-2-CC-90': 0.002118165566626695})"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}