{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for a in ['', '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python37.zip',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/lib-dynload',\n",
    "          '/Users/kristjan.roosild/.local/lib/python3.7/site-packages',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages',\n",
    "          '/Users/kristjan.roosild/projects/donkeycar',\n",
    "          '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/locket-0.2.1-py3.7.egg']:\n",
    "    sys.path.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________             ______                   _________              \n",
      "___  __ \\_______________  /___________  __    __  ____/_____ ________\n",
      "__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n",
      "_  /_/ // /_/ /  / / /  ,<  /  __/  /_/ /     / /___  / /_/ /_  /    \n",
      "/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n",
      "                                 /____/                              \n",
      "\n",
      "using donkey v4.3.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import donkeycar as dk\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "from donkeycar.config import Config\n",
    "from donkeycar.parts.keras import KerasPilot\n",
    "from donkeycar.pipeline.database import PilotDatabase\n",
    "from donkeycar.pipeline.sequence import TubRecord, TubSequence, TfmIterator\n",
    "from donkeycar.pipeline.types import TubDataset\n",
    "from donkeycar.pipeline.augmentations import ImageAugmentation\n",
    "from donkeycar.utils import normalize_image, train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchSequence(object):\n",
    "    \"\"\"\n",
    "    The idea is to have a shallow sequence with types that can hydrate\n",
    "    themselves to np.ndarray initially and later into the types required by\n",
    "    tf.data (i.e. dictionaries or np.ndarrays).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: KerasPilot,\n",
    "                 config: Config,\n",
    "                 records: List[TubRecord],\n",
    "                 is_train: bool) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.sequence = TubSequence(records)\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = ImageAugmentation(config, 'AUGMENTATIONS')\n",
    "        self.transformation = ImageAugmentation(config, 'TRANSFORMATIONS')\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(len(self.pipeline) / self.batch_size)\n",
    "\n",
    "    def image_processor(self, img_arr):\n",
    "        \"\"\" Transformes the images and augments if in training. Then\n",
    "            normalizes it. \"\"\"\n",
    "        img_arr = self.transformation.run(img_arr)\n",
    "        if self.is_train:\n",
    "            img_arr = self.augmentation.run(img_arr)\n",
    "        norm_img = normalize_image(img_arr)\n",
    "        return norm_img\n",
    "\n",
    "    def _create_pipeline(self) -> TfmIterator:\n",
    "        \"\"\" This can be overridden if more complicated pipelines are\n",
    "            required \"\"\"\n",
    "\n",
    "        # 1. Initialise TubRecord -> x, y transformations\n",
    "        def get_x(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting x from record for training\"\"\"\n",
    "            out_tuple = self.model.x_transform_and_process(\n",
    "                record, self.image_processor)\n",
    "            # convert tuple to dictionary which is understood by tf.data\n",
    "            out_dict = self.model.x_translate(out_tuple)\n",
    "            return out_dict\n",
    "\n",
    "        def get_y(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting y from record for training \"\"\"\n",
    "            y0 = self.model.y_transform(record)\n",
    "            y1 = self.model.y_translate(y0)\n",
    "            return y1\n",
    "\n",
    "        # 2. Build pipeline using the transformations\n",
    "        pipeline = self.sequence.build_pipeline(x_transform=get_x,\n",
    "                                                y_transform=get_y)\n",
    "        return pipeline\n",
    "\n",
    "    def create_tf_data(self) -> tf.data.Dataset:\n",
    "        \"\"\" Assembles the tf data pipeline \"\"\"\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator=lambda: self.pipeline,\n",
    "            output_types=self.model.output_types(),\n",
    "            output_shapes=self.model.output_shapes())\n",
    "        return dataset.repeat().batch(self.batch_size)\n",
    "\n",
    "\n",
    "def get_model_train_details(database: PilotDatabase, model: str = None) -> Tuple[str, int]:\n",
    "    if not model:\n",
    "        model_name, model_num = database.generate_model_name()\n",
    "    else:\n",
    "        model_name, model_num = os.path.abspath(model), 0\n",
    "    return model_name, model_num\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config file: /Users/kristjan.roosild/mycar/config.py\n",
      "loading personal config over-rides from myconfig.py\n"
     ]
    }
   ],
   "source": [
    "MYCAR_PATH = '/Users/kristjan.roosild/mycar/'\n",
    "DATA_PATH = '/Users/kristjan.roosild/OneDrive/kool/maka/data/'\n",
    "MODELS_LOCATION = '/Users/kristjan.roosild/OneDrive/kool/maka/models/'\n",
    "\n",
    "cfg = dk.load_config(config_path=MYCAR_PATH + 'config.py')\n",
    "\n",
    "cfg.SEQUENCE_LENGTH = 3\n",
    "cfg.WANDB_ENABLED = False\n",
    "\n",
    "def load_data(cfg, model):\n",
    "    tub_names_80_speed = [\n",
    "        '1-1-CC-80',\n",
    "        '2-1-CW-80',\n",
    "        '4-1-CC-80',\n",
    "    ]\n",
    "    tub_names_85_speed = [\n",
    "        '1-3-CC-85',\n",
    "        '2-3-CW-85',\n",
    "        '3-3-CW-85',\n",
    "        '4-3-CC-85'\n",
    "    ]\n",
    "    tub_names_90_speed = [\n",
    "        '1-2-CC-90',\n",
    "        '2-2-CW-90',\n",
    "        '3-2-CW-90',\n",
    "        '4-2-CC-90',\n",
    "    ]\n",
    "\n",
    "    def load_records(tub_name, model):\n",
    "        return TubDataset(\n",
    "            config=cfg,\n",
    "            tub_paths=[os.path.expanduser(DATA_PATH + tub_name)],\n",
    "            seq_size=model.seq_size()).get_records()\n",
    "\n",
    "    tub_records_80_speed = {tn: load_records(tn, model) for tn in tub_names_80_speed}\n",
    "    tub_records_85_speed = {tn: load_records(tn, model) for tn in tub_names_85_speed}\n",
    "    tub_records_90_speed = {tn: load_records(tn, model) for tn in tub_names_90_speed}\n",
    "\n",
    "    return tub_records_80_speed, tub_records_85_speed, tub_records_90_speed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def prep_fold_data(kl, cfg, data):\n",
    "    training_records, validation_records = train_test_split(data, shuffle=True,\n",
    "                                                            test_size=(1. - cfg.TRAIN_TEST_SPLIT))\n",
    "    print(f'Records # Training {len(training_records)}')\n",
    "    print(f'Records # Validation {len(validation_records)}')\n",
    "    # We need augmentation in validation when using crop / trapeze\n",
    "    training_pipe = BatchSequence(kl, cfg, training_records, is_train=True)\n",
    "    validation_pipe = BatchSequence(kl, cfg, validation_records, is_train=False)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset_train = training_pipe.create_tf_data().prefetch(tune)\n",
    "    dataset_validate = validation_pipe.create_tf_data().prefetch(tune)\n",
    "    train_size = len(training_pipe)\n",
    "    val_size = len(validation_pipe)\n",
    "    assert val_size > 0, \"Not enough validation data, decrease the batch size or add more data.\"\n",
    "    return dataset_train, dataset_validate, train_size, val_size\n",
    "\n",
    "\n",
    "def init_wandb(tub, kl):\n",
    "    import wandb\n",
    "    config = {\n",
    "        \"tub\": tub,\n",
    "        \"model\": str(kl)\n",
    "    }\n",
    "    wandb.init(project=\"master-thesis\", entity=\"kristjan\", config=config)\n",
    "\n",
    "\n",
    "def train(tub, kl, model_path, cfg, data):\n",
    "    dataset_train, dataset_validate, train_size, val_size = prep_fold_data(kl, cfg, data)\n",
    "    if cfg.WANDB_ENABLED:\n",
    "        init_wandb(tub, kl)\n",
    "    history = kl.train(model_path=model_path,\n",
    "                       train_data=dataset_train,\n",
    "                       train_steps=train_size,\n",
    "                       batch_size=cfg.BATCH_SIZE,\n",
    "                       validation_data=dataset_validate,\n",
    "                       validation_steps=val_size,\n",
    "                       epochs=cfg.MAX_EPOCHS,\n",
    "                       verbose=cfg.VERBOSE_TRAIN,\n",
    "                       min_delta=cfg.MIN_DELTA,\n",
    "                       patience=cfg.EARLY_STOP_PATIENCE,\n",
    "                       show_plot=cfg.SHOW_PLOT,\n",
    "                       add_wandb_callback=cfg.WANDB_ENABLED)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return np.mean((np.array(v1) - np.array(v2)) ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_loss(kl, test_records):\n",
    "    pipe = BatchSequence(kl, cfg, test_records, is_train=False)\n",
    "    steps = len(pipe)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset = pipe.create_tf_data().prefetch(tune)\n",
    "    test_preds = kl.interpreter.model.predict(\n",
    "        dataset,\n",
    "        workers=2,\n",
    "        use_multiprocessing=True,\n",
    "        steps=steps,\n",
    "        verbose=1)[0]\n",
    "    ground_truth = [r.underlying['user/angle'] for r in test_records]\n",
    "    return mse(ground_truth, test_preds)\n",
    "\n",
    "def get_test_metrics(model, tub_name, tub_records_80_speed, tub_records_85_speed, tub_records_90_speed):\n",
    "    if cfg.WANDB_ENABLED:\n",
    "        import wandb\n",
    "    test_losses_for_80_speed = {}\n",
    "    test_losses_for_85_speed = {}\n",
    "    test_losses_for_90_speed = {}\n",
    "\n",
    "    print(f'Getting 80-speed mse-s for model trained with {tub_name}')\n",
    "    for test_tub_name, test_records in tub_records_80_speed.items():\n",
    "        mse_80_speed = get_loss(model, test_records)\n",
    "        test_losses_for_80_speed[test_tub_name] = mse_80_speed\n",
    "        if cfg.WANDB_ENABLED:\n",
    "            wandb.run.summary[\"80_speed_test_loss\"] = mse_80_speed\n",
    "        print(f'80 speed mse for test tub {test_tub_name} is {mse_80_speed}')\n",
    "\n",
    "    print(f'Getting 85-speed mse-s for model trained with {tub_name}')\n",
    "    for test_tub_name, test_records in tub_records_85_speed.items():\n",
    "        mse_85_speed = get_loss(model, test_records)\n",
    "        test_losses_for_85_speed[test_tub_name] = mse_85_speed\n",
    "        if cfg.WANDB_ENABLED:\n",
    "            wandb.run.summary[\"85_speed_test_loss\"] = mse_85_speed\n",
    "        print(f'85 speed mse for test tub {test_tub_name} is {mse_85_speed}')\n",
    "\n",
    "    print(f'Getting 90-speed mse-s for model trained with {tub_name}')\n",
    "    for test_tub_name, test_records in tub_records_90_speed.items():\n",
    "        if test_tub_name == tub_name:\n",
    "            continue\n",
    "        mse_90_speed = get_loss(model, test_records)\n",
    "        test_losses_for_90_speed[test_tub_name] = mse_90_speed\n",
    "        if cfg.WANDB_ENABLED:\n",
    "            wandb.run.summary[\"90_speed_test_loss\"] = mse_90_speed\n",
    "        print(f'90 speed mse for test tub {test_tub_name} is {mse_90_speed}')\n",
    "\n",
    "    return test_losses_for_80_speed, test_losses_for_85_speed, test_losses_for_90_speed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, tub_name: str, train_records: list, cfg: Config):\n",
    "    model_name = f'{str(model)}-tub-{tub_name}'\n",
    "    model_path = f'{MODELS_LOCATION}{model_name}.h5'\n",
    "    train(tub_name, model, model_path, cfg, train_records)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# model = dk.parts.keras.KerasLinearOnlySteering()\n",
    "# tub_records_80_speed, tub_records_85_speed, tub_records_90_speed = load_data(cfg, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.parts.keras:input_shape (120, 160, 3) num_outputs 1\n",
      "2022-03-01 17:52:57.490902: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-03-01 17:52:57.518005: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fead026fcd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-01 17:52:57.518020: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:donkeycar.parts.keras:Created Keras3D_CNNOnlySteering with interpreter: KerasInterpreter\n",
      "INFO:donkeycar.parts.keras:Num outputs 1\n",
      "INFO:donkeycar.parts.keras:Sequence length 3\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-1-CC-80']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-1-CC-80/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-1-CW-80']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-1-CW-80/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-1-CC-80']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-1-CC-80/catalog_5.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-3-CC-85/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-3-CC-85']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-3-CW-85']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-3-CW-85/catalog_6.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/3-3-CW-85']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/3-3-CW-85/catalog_4.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-3-CC-85/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-3-CC-85']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/1-2-CC-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/1-2-CC-90/catalog_4.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/2-2-CW-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-2-CW-90/catalog_2.catalog\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/3-2-CW-90/catalog_6.catalog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/3-2-CW-90']\n",
      "INFO:donkeycar.pipeline.types:Loading tubs from paths ['/Users/kristjan.roosild/OneDrive/kool/maka/data/4-2-CC-90']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/4-2-CC-90/catalog_7.catalog\n"
     ]
    }
   ],
   "source": [
    "cfg.SEQUENCE_LENGTH = 3\n",
    "\n",
    "model = dk.parts.keras.Keras3D_CNNOnlySteering(seq_length=cfg.SEQUENCE_LENGTH)\n",
    "tub_records_80_speed, tub_records_85_speed, tub_records_90_speed = load_data(cfg, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, <donkeycar.parts.keras.Keras3D_CNNOnlySteering at 0x7feb28eece90>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_outputs, model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 17223\n",
      "Records # Training 13778\n",
      "Records # Validation 3445\n",
      "Epoch 1/100\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0369\n",
      "Epoch 00001: val_loss improved from inf to 0.42340, saving model to /Users/kristjan.roosild/OneDrive/kool/maka/models/Keras3D_CNNOnlySteering-tub-all90speed.h5\n",
      "108/108 [==============================] - 231s 2s/step - loss: 1.0369 - val_loss: 0.4234\n",
      "Epoch 2/100\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4212\n",
      "Epoch 00002: val_loss improved from 0.42340 to 0.38696, saving model to /Users/kristjan.roosild/OneDrive/kool/maka/models/Keras3D_CNNOnlySteering-tub-all90speed.h5\n",
      "108/108 [==============================] - 241s 2s/step - loss: 0.4212 - val_loss: 0.3870\n",
      "Epoch 3/100\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.2420\n",
      "Epoch 00003: val_loss improved from 0.38696 to 0.36367, saving model to /Users/kristjan.roosild/OneDrive/kool/maka/models/Keras3D_CNNOnlySteering-tub-all90speed.h5\n",
      "108/108 [==============================] - 251s 2s/step - loss: 0.2420 - val_loss: 0.3637\n",
      "Epoch 4/100\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.1671\n",
      "Epoch 00004: val_loss did not improve from 0.36367\n",
      "108/108 [==============================] - 243s 2s/step - loss: 0.1671 - val_loss: 0.3697\n",
      "Epoch 5/100\n",
      " 48/108 [============>.................] - ETA: 2:08 - loss: 0.1382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/8pmrmvpd47j_nm5l2j2_d9gh0000gp/T/ipykernel_55525/2928635681.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'len {len(all_records)}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'all90speed'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mall_records\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/w9/8pmrmvpd47j_nm5l2j2_d9gh0000gp/T/ipykernel_55525/1436584345.py\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(model, tub_name, train_records, cfg)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mmodel_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{str(model)}-tub-{tub_name}'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{MODELS_LOCATION}{model_name}.h5'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtub_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_records\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/w9/8pmrmvpd47j_nm5l2j2_d9gh0000gp/T/ipykernel_55525/4001876378.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(tub, kl, model_path, cfg, data)\u001B[0m\n\u001B[1;32m     40\u001B[0m                        \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcfg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEARLY_STOP_PATIENCE\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m                        \u001B[0mshow_plot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcfg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSHOW_PLOT\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m                        add_wandb_callback=cfg.WANDB_ENABLED)\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mhistory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/donkeycar/donkeycar/parts/keras.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, model_path, train_data, train_steps, batch_size, validation_data, validation_steps, epochs, verbose, min_delta, patience, show_plot, add_wandb_callback)\u001B[0m\n\u001B[1;32m    185\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m             \u001B[0mworkers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m             use_multiprocessing=False)\n\u001B[0m\u001B[1;32m    188\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mshow_plot\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m    846\u001B[0m                 batch_size=batch_size):\n\u001B[1;32m    847\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 848\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    849\u001B[0m               \u001B[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    850\u001B[0m               \u001B[0;31m# This blocks until the batch has finished executing.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    578\u001B[0m         \u001B[0mxla_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    579\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 580\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    581\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtracing_count\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    609\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 611\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    612\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    613\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2418\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2419\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2420\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2421\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2422\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   1663\u001B[0m          if isinstance(t, (ops.Tensor,\n\u001B[1;32m   1664\u001B[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001B[0;32m-> 1665\u001B[0;31m         self.captured_inputs)\n\u001B[0m\u001B[1;32m   1666\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1667\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1744\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1745\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1746\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1747\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    596\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    599\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "all_records = []\n",
    "for records in tub_records_90_speed.values():\n",
    "    all_records.extend(records)\n",
    "print(f'len {len(all_records)}')\n",
    "\n",
    "model = train_model(model, 'all90speed', all_records, cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "#\n",
    "# losses_3d = defaultdict(dict)\n",
    "#\n",
    "# for tub_name, records in tub_records_90_speed.items():\n",
    "#     model = train_model(model, tub_name, records[:220], cfg)\n",
    "#\n",
    "#     test_losses_for_80_speed, test_losses_for_85_speed, test_losses_for_90_speed = get_test_metrics(\n",
    "#         model, tub_name, tub_records_80_speed, tub_records_85_speed, tub_records_90_speed\n",
    "#     )\n",
    "#\n",
    "#     losses_3d.update(test_losses_for_80_speed)\n",
    "#     losses_3d.update(test_losses_for_85_speed)\n",
    "#     losses_3d.update(test_losses_for_90_speed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}